# RNN-based-Speech-noise-reduction-android-app
基于递归神经网络的语音降噪系统设计
=================

*   **项目背景**

生活中有很多让我们交流起来比较困难的情况：比如觥筹交错的鸡尾酒会、车流众多的公路上、正在施工的工地上等等，都会因为周围的噪声导致两个人交流时需要很大声地讲话才能勉强听清。

考虑到智能手机在生活已经普及，蓝牙耳机也越来越普遍，我们通过手机和蓝牙耳机再加一款app来解决上述问题。以手机麦克风采集到的声音为源，将有效语音从干扰背景中分离出来，增强语音品质和清晰度，以解决上述问题。

*   **算法说明**

1.  **概述**

语音噪声抑制从70年代起就是人们感兴趣的话题。尽管在一些特定场合降噪的质量有了显著的效果，但传统的算法主要依赖于谱估计技术，也就是噪声谱估计器，其本质是由语音活动检测器(VAD)或类似算法驱动，主要包括三个组件：声音活动检测模块、谱估计模块、谱减模块，这3个组件都需要精确的估计值，并且很难进行调优。

近年来，机器学习的相关算法被用于语音降噪领域，并取得了很好的效果，可以参考文章【1】【2】【3】【4】，但是绝大部分的算法需要很高的硬件资源和GPU运算能力，同时大部分应用并没有考虑低延时的场景。因此我们设计了一种算法，结合二者的优势，采用递归神经网络（RNN），在保证一定的降噪效果的前提下，降低了网络复杂程度和硬件需求，同时实现了实时的效果，在降噪质量上也优于传统的基于谱减法等降噪算法。

2.  **模型**

我们使用了一种噪声抑制的混合方法，使用深度学习来抑制需要仔细调优的噪声，同时对不需要调优的部件使用基本信号处理构建。

1.  基本信号处理部分

循环处理的窗口大约20ms左右，其中有10ms左右相互重叠，窗口定义为， 其中N为窗口长度。

我们感知声音的频率刻度可以分为22个波段，而不是480个复波谱值【6】，但是我们不可能仅仅从22个频段的能量还原出音频信号，因此我们实际计算的是这些波段的增益量，这样做的优点有以下几个方面：

*   有限的几个波段，大大降低了模型复杂度
*   降低生成musical-noise的可能
*   增益在\[0，1\]区间，可以用简单sigmoid激活函数避免引入其他我们生成噪声

具体的计算如下：定义每个波段的能量函数，增益可以定义为，也就是理想信号和含噪声信号的能量比值，在\[0,1\]之间。

由于我们频段的频率分辨率太粗，无法滤除音调谐波之间的噪声，我们使用基本信号处理。当人们对同一个变量进行多次测量时，提高精度（降低噪声）的最简单方法就是计算平均值。显然，仅仅计算相邻音频样本的平均值并不是我们想要的，因为它会导致低通滤波。然而，当信号是周期性的（例如浊音）时，我们可以计算偏移音高周期的样本的平均值。这导致梳状滤波器它可以让谐波通过，同时衰减它们之间的频率 \- 噪声所在的位置。为了避免信号失真，梳状滤波器对每个频带独立应用，其滤波器强度取决于音调相关性和神经网络计算的频带增益。

我们采用FIR滤波器进行音调滤波，但也可以使用IIR滤波器，如果强度过于激进，这将导致更大的噪声衰减，存在更高失真的风险。

我们为神经网络提供的输入有42个特征，包括22个频段的倒谱变换的系数，跨帧倒谱系数关于时间的6个一阶导数和6个二阶导数用于增加跨帧间的联系，以及6个频段的音高增益值，还有1个基音周期和1个特殊的非平稳值（用于检测）。

音高增益的计算公式如下：

1.  神经网络算法

递归神经网络（RNN）在这里非常重要，因为它们可以对时间序列进行建模，而不是仅仅考虑输入和输出帧。这对噪声抑制尤为重要，因为需要时间来估算噪声。长期以来，RNN的能力受到很大限制，因为它们无法长时间保持信息，并且因为在通过时间反向传播时所涉及的梯度下降过程非常低效（消失的梯度问题）。通过门控单元GRU的发明（图2）解决了这两个问题.

神经网络的拓扑如图1所示，每个矩形框是一层，数字表示神经元的个数，对应三个模块也是之前概述中所述的三个模块

图1

Dense表示全连接层。

GRU层的每个神经元包含两个栅极，复位栅极和更新栅极，如图所示：

图2

我们定义了损失函数为，γ表示对噪音的容忍程度，我们取值为0.25。

关于实验数据，与语音识别常见的不同，我们选择不对我们的特征应用倒谱均值归一化，并且我们保留表示能量的第一个倒谱系数。因此，我们必须确保数据包含所有实际级别的音频。我们还对音频应用随机滤波器，使系统对各种麦克风频率响应（通常由倒谱均值归一化处理）具有鲁棒性。

在6G的数据集下，神经网络的训练时长30个小时左右。

我们将python转化为c之后，模型所占空间只有不到100kb，并且处理速度满足实时处理的需求，时延低于0.5s，而且通过良好的矢量化（SSE / AVX），应该可以使其比目前快4倍。

*   **关键代码说明**

原始代码文件中的各个函数有详细的注释，现将关键内容整理如下。

**onCreate**：初始化按钮；

**onClick****：**设置按钮触发事件

*   录制按钮：对麦克风的音频进行录制。触发startAudioRecord函数和stopAudioRecord函数，将结果保存为pcm文件
*   播放录制音频按钮：对麦克风录音进行播放。触发startAudioPlay函数和stopAudioPlay函数 读取原始pcm文件并进行播放
*   降噪处理按钮：对音频进行降噪处理。触发startAudioTran函数，输入为原始pcm文件，输出为降噪处理后的pcm文件
*   实时处理按钮：实时进行处理。触发为startRealTimeAudioTran函数。
*   开启/关闭实时降噪按钮：实时降噪效果的开关。修改Flag传给实时处理函数。

**startAudioRecord****：**

*   检测权限
*   开始RecordTask线程 调用AudioRecord；

**startAudioPlay****：**

*   开始PlayTask线程 调用AudioTrack；

**startAudioTran****：**

*   开启TranTask线程 调用rnnoise函数；

**startAudioTran****：**

*   开始RealTimeRecordTask线程 循环录取麦克风数据并将结果放入原始录音队列；
*   开始RealTimeTranTask线程 循环从原始录音队列读取数据进行处理并将处理结果放到播放队列；
*   开始RealTimePlayTask 循环从录音队列读取数据并调用AudioTrack播放；

降噪部分的rnnoise使用c语言实现。代码位于cpp文件夹下。封装为rnnoise降噪函数。输入为pcm文件，输出为处理后的pcm文件。

代码逻辑如下：

*   从原始pcm文件读取一帧音频；
*   对该帧音频计算特征。特征包括 22个Bark尺度，跨帧的前6个系数的一阶和二阶导数，基音周期（1 /基频），6频段的音高增益（发声强度）和一种特殊的非平稳值，可用于检测语音；
*   将特征传入rnn计算函数，根据预训练的网络执行forward和GRU单元。 输出为Bark尺度上的增益；
*   根据rnn输出计算出该帧输出的音频，写入结果中。

实时处理逻辑：

*   从录音线程获取音频buffer；
*   对buffer进行字节序变换（java默认大尾端存储short类型，而c默认小尾端） （重要）；
*   对buffer加上上一个buffer的最后一帧（480个short）（用于保证每一小段音频之间的连续性）（重要）；
*   将结果放入录音队列；
*   实时处理函数从录音队列读取一个buffer，交给rnnoise处理，将处理出的结果进行字节序变换，放入播放队列；
*   实时播放函数从播放队列中读取buffer进行播放。
*   **界面说明**

1.  **软件图标**

软件名称为“语音降噪”，图标如图所示。

2.  **软件启动页面**

软件启动页面如图所示，打开软件时显示，持续时间4秒。

3.  **软件主界面**

软件主界面如图所示，包括两个模块：

1.  录音降噪模块

此模块为实现录音音频文件降噪而设计。包括四个按钮，按照逻辑先后顺序介绍如下。

1.  点击“开始录制”即可启动麦克风收集语音，再次点击即可停止；
2.  随后点击“播放录制音频”即可开始播放未处理的原始录音；
3.  随后点击“降噪处理”即可完成降噪操作；
4.  最后点击“播放处理音频”即可播放降噪处理后的音频。
5.  实时降噪模块

此模块为实现实时音频降噪而设计。包括两个按钮，按照逻辑先后顺序介绍如下。

1.  佩戴蓝牙耳机，点击“实时返送”即可开始讲话，耳机中播放同步音频；
2.  讲话过程中，点击“开启实时降噪”即可开始降噪，耳机中播放同步降噪后音频，再次点击切换回原始音频，如此反复；
3.  点击“停止实时转换”即可结束模块。

*   **参考文献**

【1】A. Maas, Q.V. Le, T.M. O’Neil, O. Vinyals, P. Nguyen, and A.Y. Ng,

“Recurrent neural networks for noise reduction in robust ASR,” in Proc.

INTERSPEECH, 2012.

【2】D. Liu, P. Smaragdis, and M. Kim, “Experiments on deep learning

for speech denoising,” in Proc. Fifteenth Annual Conference of the

International Speech Communication Association, 2014.

【3】Y. Xu, J. Du, L.-R. Dai, and C.-H. Lee, “A regression approach to speech

enhancement based on deep neural networks,” IEEE Transactions on

Audio, Speech and Language Processing, vol. 23, no. 1, pp. 7–19, 2015.

【4】A. Narayanan and D. Wang, “Ideal ratio mask estimation using deep

neural networks for robust speech recognition,” in Proc. ICASSP, 2013,

7096.  7092–7096.

【5】S. Mirsamadi and I. Tashev, “Causal speech enhancement combining

data-driven learning and suppression rule estimation.,” in Proc. INTERSPEECH, 2016, pp. 2870–2874

【6】B.C.J. Moore, An introduction to the psychology of hearing, Brill, 2012
